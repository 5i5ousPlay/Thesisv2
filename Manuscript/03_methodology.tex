\chapter{METHODOLOGY}

This study aims to derive and test various methods of location approximation based on the LSA algorithm. Two proposed methods in particular will be discussed in the following sections. Each method uses the LSA algorithm as a semantic similarity based location approximation of non-geotagged tweets.  As mentioned in Chapter I, all methods are implemented using Python and its extended library, Gensim. Also, the output values of all these methods will be outputted as a JSON file that will be passed on to an existing API or platform for visualization. 

\section{LDA-LSA Combination}


The first proposed method involves using another algorithm in addition to the LSA algorithm, which is the Latent Dirichlet Allocation algorithm.

\subsection{Logic Behind Combining LDA and LSA}

The Latent Semantic Analysis algorithm analyzes a given document of words and compares that said document to an entire collection of other documents to determine which documents that particular query is most similar to in terms of meaning. LSA takes into account factors such as grammar, word usage, and sentence structure to determine which documents or texts convey the same message. A query is given a similarity score to each document in the collection it is being analyzed against which serves as a measure of how similar the meaning of the query is to the meaning of another given text. 

The Latent Dirichlet Allocation algorithm on the other hand takes an entire collection of documents and generates topics based on the underlying themes that can be found in each document in the collection. LDA generates topics in the form of word clouds. Each word in a particular word cloud has a score which represents the weight of that word to that particular topic. In this way, words that point toward a possible point of interest in analysis are grouped together. 

The first proposed methodology seeks to use these two algorithms in succession to test if doing so would help produce a more fine-tuned location approximation model for disaster-related tweets. LDA is implemented first followed by LSA on the collection of words from the dataset mentioned in section 1.4. This methodology first attempts to identify the possible topics Filipinos tweet about before performing semantic analysis. This means that instead of analyzing a given query against all documents in the collection, it will only be analyzed against the documents that contribute to the topic it is most semantically similar to. This allows a given query to be processed using a set of tweets that pertain to just a single theme instead of a wide variety of possible topics. 

\subsection{LDA Model Creation}

The collection of geotagged tweets is first cleaned by removing stop words and words that occur less than 1 percent of the time in the entire collection. The resulting tokens are then inserted into a dictionary as to make it easier to identify the unique words in the collection. The populated dictionary is transformed to a corpus of words afterwards. Both the populated dictionary and transformed corpus are fed to an LDA model created via Python’s Gensim library. The created LDA model will generate only ten topics (tentative) as opposed to the recommended 100-200 since there are generally only a few topics present during a disaster event. 

\subsection{First LSA Model Creation}

Word clouds are formed from all the topics generated by the LDA model by extracting the top ten contributing words for each topic and storing them in a Python list. This list of word clouds is stored in another dictionary and corpus of its own. Both the dictionary and corpus of the word clouds are fed to an LSA model also created via Python’s Gensim library. Afterwards, the corpus of word clouds is wrapped around the created LSA model and is fed to a similarity matrix in preparation for semantic similarity queries. This first LSA model is used to determine which topic a given non-geotagged query is most semantically similar to.

\subsection{Second LSA Model Creation}

Another LSA model is created which uses the original corpus and dictionary of words containing the tokens for all the geotagged tweets in the dataset. The original corpus of words is wrapped around the model and is fed to an accompanying similarity matrix. This second LSA model is used in retrieving the necessary semantic similarity scores for the geolocation approximation of a given query. After identifying which topic a given non-geotagged query is most semantically similar to, the tweets which contribute to that found topic are extracted along with their semantic similarity scores to a particular non-geotagged query.

\subsection{Location Approximation of Non-Geotagged Queries}

Non-geotagged tweets are accepted as queries, and each query is compared to the first similarity matrix to return the similarity score of said query to all the word clouds. In other words, each non-geotagged tweet will be processed to see which topic it is most semantically similar to. The topic with the highest similarity score to a given query is returned then the original corpus of tokens from the geotagged tweets is searched to see which tweets contain any of the top ten contributing words to the said topic. The given query is then processed to the second LSA model and second similarity matrix to retrieve its semantic similarity score against the tweets that contribute to the retrieved topic. The latitude/longitude coordinates of the top ten most semantically similar tweets to the given query among the extracted geotagged tweets are stored in a list and are used to approximate the location of the corresponding non-geotagged tweet query. This entire process will be done for all the non-geotagged tweets in the dataset. 

\section{Dynamic Dictionary Per Geographic Region}

The second proposed method deals with using a set of dynamic dictionaries as reference when it comes to approximating the location of a non-geotagged tweet. This proposed method involves the usage of up to as many geographic regions as there are in the Philippines.
A dynamic dictionary is used per geographic region, meaning there are 17 different dynamic dictionaries that a non-geotagged tweet is compared with for semantic similarity in order to approximate its location. This approach builds on the findings of the study discussed in section 2.5.2; that narrowing the possible approximated location of a non-geotagged tweet to a certain region or area improves geolocation accuracy \cite{VELASCOBERMEJODOMINGO2018}. This method also operates behind the presupposition that people from a particular region or area has a distinct way of talking/tweeting \cite{ROSALES2017, VELASCOBERMEJODOMINGO2018}.

\subsection{Region Assignment of A Given Geolocated Tweet}

First, the geographic boundaries of each Philippine region are identified in terms of latitude/longitude coordinates. This is done with the help of a shapefile which contains all the geographic regions of the Philippines and stores each as a multi-polygon object. A multi-polygon object contains an array of array of points that define the boundaries of a set of closed figures. The geotagged tweets are processed using a Python library called Geopandas in order to assign each tweet to its corresponding geographic region based on its latitude/longitude coordinates. To accomplish this, the shapefile and the dataset containing the geolocated tweets are first converted to geojson files. The two resulting geojson files are then merged using a Python Geopandas function called 'spatial join' to determine which region (using their latitude/longitude coordinates) a given geolocated tweet belongs in. Performing 'spatial join' returns a list where each element contains the list of indices of the geolocated tweets that belong in that integer-represented region. Collections representative of each region are created to store all the geolocated tweets in their appropriate integer-represented region. 

\subsection{Creating Dynamic Dictionaries For Each Region}

All 17 collections of geotagged tweets for each region are transformed to their own dictionaries and corpuses. All dictionaries and corpuses are fed to a corresponding LSA model representative of each Philippine geographic region. All corpuses are wrapped around their corresponding LSA model and are fed to similarity matrices of their own. Non-geotagged tweets are then accepted as queries. Each query is processed by comparing said non-geotagged tweet to all the available LSA models and similarity matrices. The LSA model, along with its corresponding similarity matrix, that returns the ten highest similarity scores to the query is selected as the approximate region that given query belongs to. That LSA model is extracted, and the given non-geotagged query’s similarity scores with the top ten most semantically similar geolocated tweets to it in the corpus of that corresponding model are retrieved and used to approximate the location of the given query. The dictionaries are updated after a certain number of non-geotagged queries have been processed—perhaps after every 500 tweets. Incoming geolocated tweets and formerly non-geotagged tweets with approximated locations are inserted to their corresponding dictionaries, corpuses, and models. All dictionaries, corpuses, and models are then re-instantiated to reflect the changes in content. This is done to simulate an increasingly rich set of models for potentially more accurate geolocation.

\section{Method of Longitude/Latitude Approximation}

The weighted arithmetic mean formula is used to approximate the weighted center of each convex hull formed for each non-geotagged query for both methodologies. Two computations, one for latitude and one for longitude, will be done. The value of each term is the retrieved coordinate (latitude or longitude), and the weight of the said term is its semantic similarity score with the queried tweet. The sum of all these terms is  divided into the sum of the weights of all terms to retrieve the approximated latitude and longitude of the processed non-geotagged tweet.

The logic behind using the weighted arithmetic mean formula to construct the convex hull in which a particular approximated tweet lies in stems from the same idea as to why the LSA algorithm was used for this study. The weighted arithmetic mean formula allows a given query tweet to be analyzed relative to its semantic similarity scores to a given set of geolocated tweets. The typical way of approximating the center of a given polygon is by getting the average of all its coordinates. That way of measurement is not effective for the aims of this study, as it does not take into account the semantic similarity scores of a particular query tweet against the processed tweets of the dataset. The weighted arithmetic mean formula however, factors in the 'weight' or the degree of influence a particular value has in the approximation of another value. This essentially allows a non-geotagged query's approximated latitude and longitude coordinates to be influenced by geolocated tweets in accordance to the assumption of this study that the more semantically similar a given tweet is to another, the more likely the locations of where both tweets were tweeted by their respective users are closer to each other. The multiplication of the semantic similarity score of a query tweet to a geolocated tweet to the latter's latitude/longitude values reflects this idea in the actual geolocation  process.





%\section{Different Sources of Data}

%The third method is more concerned with the geotagged data more than anything else. This study will initially use mined tweets from Twitter, but this approach will attempt to explore other sources of data such as Facebook and other sites. Users on Facebook also have the capacity to indicate their location with posts, either via making a public post or making a check in. Although it may be more complicated than it seems to mine data from Facebook because of recent legal incidents, it is still another site that can be used as a good source of crowdsourced data. There are also hashtags on Facebook similar to Twitter. Other mining tools will be used for sites like Facebook, but the process will be the same by extracting the located posts, configuring them into a dictionary, then a TF-IDF matrix and an LSA model. In this way, the data from Facebook and other sites will be combined with the data from Twitter for a theoretically more accurate dictionary. 