\chapter{RECOMMENDATIONS}
The results of this study indicate that there is still more room for improvement with regards to Latent Semantic Analysis based geolocation. It was mentioned in the results section that the average distance   the approximated and actual location of the processed tweets is over two hundred kilometers for both methodologies. Two hundred kilometers is still quite a large gap especially for disaster-related tweets. It is important that accurate disaster-related information be disseminated quickly and efficiently in order to help respondents act accordingly. That being said the following sections will discuss several possible modifications to the methodologies of this study that may potentially fine-tune them further.

\section{LDA-LSA Double Filter}
A possible modification to the first methodology is concerned with choosing a different number of topics for training all models. One hundred and two hundred topics were specified for the Latent Dirichlet Allocation models and Latent Semantic Analysis models respectively. The main reason for choosing these numbers of topics for the models is to follow Python conventions. Two hundred topics is the recommended number for training Python models in general as discussed in the official Gensim documentation. This number was lessened for the LDA models based on the observation that there is usually a limited number of disaster-related topics during times of disaster. Disaster-related tweets also usually do not contain many sub-topics. 

It is possible that there is a more optimal number of topics that may be used to train LDA and LSA models which will yield more accurate results. It may be argued that training models with less topics, especially the LDA model, is reasonable behind the observation that disaster topics tend to be specific and usually do not have many branching sub-topics. In this case the algorithm will have less iterations which may improve its overall run    time and allow more queries to be processed. Training the models with less topics will also lessen the likelihood that duplicate or synonymous topics
will be generated. This may help polish the algorithm further as less redundant topics will be considered for each query.

\section{Dynamic Dictionaries Per Region}
The structure of the dataset used in this study may have impacted the overall performance of the second methodology as mentioned in
the results section. The models representative of the Philippine geographic regions are not equally refined as a result of an unequal distribution of tweets from each region. This results in cases where the distance between the actual and approximated 
location of a given query is considerably large. Incorrectly identifying the region of a tweet causes this problem since the Philippines is an archipelago and each region covers a vast area. The distance between two Philippine regions may span from tens to hundreds of kilometers.

A possible way to address this scenario is to train models representative of smaller spaces such as cities/provinces and districts. Doing so may lessen the distance between the actual and approximated location for some queries. Classifying a query to an incorrect area may not always drastically influence its approximated coordinates because it is possible that the actual location of the said query is not that far from the approximated one. However it is worth noting that training more models would result in more poorly refined models since the distribution of tweets in the dataset is not equally distributed around the Philippines. There may be models that will not contain tweets at all, and some may contain only a handful. But this may be the overall more optimal approach especially when dealing with bigger data as more areas will be represented by a particular LSA model. This allows queries to be processed with more specific information as the algorithm will consider more geographic spaces.
