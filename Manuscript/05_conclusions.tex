\chapter{CONCLUSIONS}
The results of this study show improvements from the results of previous studies regarding the location approximation of disaster-related tweets using Latent Semantic Analysis. However, the structure of the dataset of tweets used may have influenced just exactly how big these improvements actually are. Nevertheless, the implementation and execution of both methodologies showed that there are different approaches that may be used alongside Latent Semantic Analysis in the context of location approximation. 

It was discussed in the introduction section of this paper that the main question this study aimed to address is concerned with exploring different methods for a more fine-tuned geolocation approximation algorithm. The results presented in the previous section offer an answer to this said question. Perhaps generating topics from the available tweets and classifying tweets according to specific geographic regions may be a possible starting point for further fine-tuning of LSA based geolocation. The results of both methodologies also offer possible answers to the sub-questions mentioned in the introduction section.

\section{Modifications Of LSA Based Location Approximation}
The first sub-question mentioned in the introduction section of this study dealt with what possible modifications to the LSA algorithm may be made to produce a more fine-tuned approach to the geolocation of disaster-related tweets. Both methodologies address this particular sub-question.

The first methodology (LDA-LSA Double Filter) of this study was concerned with first using Latent Dirichlet Allocation to extract underlying themes in the dataset of tweets and performing Latent Semantic Analysis based on said themes. Generating possible topics from a set of tweets and processing each tweet according to the topic/s it contributes to makes for a more structured approach. The tweets were essentially grouped according to topics so that LSA will be performed using the topics as reference as opposed to iterating over the entire dataset for each incoming query. Iterating over the generated topics means that it is more likely that the tweets that will be included in the geolocation of a given query will be related to the said query in terms of meaning. Analyzing a particular tweet relative to a set of other tweets that pertain to a common topic follows the presupposition of this study that semantically similar tweets most likely come from the same general location. If a disaster event takes place in a given area then disaster-related tweets from citizens near that said area will more or less have a specific common subject. This was the main principle behind this specific modification to the LSA approach.

The second methodology (Dynamic Dictionaries Per Region) on the other hand builded on the recommendations of previous Latent Semantic Analysis based studies. The particular recommendation essentially stated that narrowing the area of where to consider tweets that will be included in the geolocation of a given query may yield more favorable results. This was done by training LSA models representative of each Philippine geographic region. Identifying which region a query tweet most likely belongs to makes iterating over the entire dataset unnecessary. Instead only the tweets that belong in the selected region are considered and used in analysis. Using LSA on its own to analyze the tweets in the dataset would entail considering tweets from Luzon, Visayas, and Mindanao to approximate the coordinates of a given query. This may be problematic in some cases since the Philippines is an archipelago and the distance between the three island groups of the country is vast. As an example, suppose tweets from Luzon and Mindanao are used to approximate the location of a given query tweet. The resulting value would be skewed because the convex hull that will be constructed to approximate the query's coordinates covers a very large area. Narrowing the area enclosed by the convex hull of tweets lowers the likelihood of this potential occurrence. 

\section{Visualization of Results and Availability For External Programs}
All LSA and LDA models were implemented and trained in Python. The trained models may be imported to any Python program with access to the Python library Gensim. Once imported, the models may be used for queries or even retrained within the program. The Python library Geopandas is also required to replicate the steps of the second methodology that is concerned with classifying a given tweet to a particular Philippine geographic region. 

As of now it is unknown if there are any other programming languages or platforms that may access the trained models completely. The official documentation for the Gensim and Geopandas libraries are only defined for Python.

As mentioned in the results section, the visualizations of the results of both methodologies of this study were created in Python as well. The Python libraries Geopandas and Matplotlib were used to create all the corresponding visualizations and figures. Any program that has access to these two Python libraries will be able to recreate the plots shown in the results section. 

%Upon presenting your results, the conclusion is where you will now %tie up these results with the original intent of the study, as %indicated by the research questions given in the Introduction. It is %in this section where you will also discuss any difficulties or %issues encountered during the study, as well as your recommended %method for addressing these problems.

%The general way to organize your conclusion is to present each %research sub-question as a subsection, and thoroughly answer each of %them by interpreting your results with respect to the question. With %these answered, you may then tie up all of your findings in each %subsection to answer your main research question, providing any %needed additional information or explanation. Last on the list would %be your unsolved issues and difficulties, presenting them as avenues %to motivate continued work on your chosen topic.
